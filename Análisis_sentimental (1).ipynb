{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC6-3eQc-9EC"
      },
      "source": [
        " **Aplicación de Machine Learning al análisis de sentimientos:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEzWUldUAl_p"
      },
      "source": [
        "# 0.Contexto inicial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCW3UTmP_Jeh"
      },
      "source": [
        "Esta rutina se basa en el capítulo 8 del libro *Machine Learning with PyTorch and Scikit-Learn* de Raschka. En el cual se pretende profundizar en un subcampo del Procesamiento de Lenguaje Natural (NLP), llamado **Análisis de Sentimientos**.\n",
        "\n",
        "\"El análisis de sentimiento se refiere al uso de procesamiento de lenguaje natural, análisis de texto y lingüística computacional para identificar y extraer información subjetiva de los recursos.\"\n",
        "\n",
        "El propósito específico es clasificar documentos a partir de los sentimientos sugeridos por la actitud del escritor.\n",
        "\n",
        "\n",
        "En este ejercicio se usará un dataset compuesto por 50.000 reseñas cinematográficas extraídas de la plataforma *Internet Movie Database* (IMDb) para construir un predictor que pueda distinguir entre una reseña positiva y una negativa. Es decir, es un problema de clasificación, por ende, es aprendizaje supervisado, así que se esperan los datos etiquetados.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Lw8Z-tRCYxk"
      },
      "source": [
        "En el libro se definen las siguientes etapas para el proyecto:\n",
        "\n",
        "\n",
        "1.   Limpieza y preparación de datos de texto\n",
        "2.   Construcción de 'feature vectors' (vector de características)\n",
        "3. Entrenamiento del modelo para clasificar las reseñas\n",
        "\n",
        "4. Inferencia de temas a partir de colecciones de documentos para su categorización\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP9XvrUVDwEz"
      },
      "source": [
        "# 1. Limpieza y preparación de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kSTu8D1FB7m"
      },
      "source": [
        "Lo primero será importar las librerías necesarias para el proyecto, con las versiones que se trabajan en el libro.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBbZQsQKE_73"
      },
      "source": [
        "## Importar librerías"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V8pUuiVJISc"
      },
      "source": [
        "### Instalación y verificación de versiones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqKuvAC3HLj5",
        "outputId": "eab88c8f-58a4-49fd-b4b5-e73d7546dd72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyprind==2.11.3 in /usr/local/lib/python3.10/dist-packages (2.11.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyprind==2.11.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fojyOxbQHHf2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import pyprind\n",
        "import nltk\n",
        "\n",
        "# Verificar versiones instaladas\n",
        "def check_version(library, required_version):\n",
        "    try:\n",
        "        current_version = library.__version__\n",
        "        if current_version == required_version:\n",
        "            print(f'{library.__name__} está instalado y tiene la versión {current_version}.')\n",
        "        else:\n",
        "            print(f'{library.__name__} está instalado, pero la versión {current_version} no cumple con los requisitos. Se requiere la versión {required_version}.')\n",
        "    except AttributeError:\n",
        "        print(f'{library.__name__} no está instalado.')\n",
        "\n",
        "# Verificar e instalar numpy\n",
        "check_version(np, '1.21.2')\n",
        "if '1.21.2' not in np.__version__:\n",
        "    !pip install numpy==1.21.2\n",
        "\n",
        "# Verificar e instalar pandas\n",
        "check_version(pd, '1.3.2')\n",
        "#if '1.3.2' not in pd.__version__:\n",
        "    #!pip install pandas==1.3.2\n",
        "\n",
        "# Verificar e instalar sklearn\n",
        "check_version(sklearn, '1.0')\n",
        "if '1.0' not in sklearn.__version__:\n",
        "    !pip install scikit-learn==1.0\n",
        "\n",
        "# Verificar e instalar pyprind\n",
        "check_version(pyprind, '2.11.3')\n",
        "if '2.11.3' not in pyprind.__version__:\n",
        "    !pip install pyprind==2.11.3\n",
        "\n",
        "# Verificar e instalar nltk\n",
        "check_version(nltk, '3.6')\n",
        "if '3.6' not in nltk.__version__:\n",
        "    !pip install nltk==3.6\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9VxtL3_JL2U"
      },
      "source": [
        "### Importación directa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TySWBdAEJD-D"
      },
      "outputs": [],
      "source": [
        "#!pip install pyprind==2.11.3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import pyprind\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojkwXvc0HPpw"
      },
      "source": [
        "## Importar datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THgb5z4MKp2F"
      },
      "source": [
        "Primero se hará subiendo la base de datos localmente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEDz91zSCIhg"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/movie_data.csv', encoding='utf-8')\n",
        "## El parámetro encoding en Python se utiliza para especificar\n",
        "##la codificación de caracteres al leer o escribir archivos de texto.\n",
        "##Indica cómo deben interpretarse los bytes para convertirlos\n",
        "## en caracteres y viceversa.\n",
        "\n",
        "### UTF-8: 'utf-8' (predeterminado)\n",
        "### Codificación de ancho variable que puede representar cada carácter\n",
        "### en el conjunto de caracteres.\n",
        "\n",
        "# the following is necessary on some computers:\n",
        "df = df.rename(columns={\"0\": \"review\", \"1\": \"sentiment\"})\n",
        "\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CklXvF2Kts3"
      },
      "source": [
        "Alternativamente se usará la versión publicada en la web desde un drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "BuhZ-TNaIkl_",
        "outputId": "5965fed9-1faf-413d-cdb1-9c3fb77ca49f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment\n",
              "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
              "1  OK... so... I really like Kris Kristofferson a...          0\n",
              "2  ***SPOILER*** Do not read this, if you think a...          0\n",
              "3  hi for all the people who have seen this wonde...          1\n",
              "4  I recently bought the DVD, forgetting just how...          0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7b2f7b98-2210-474c-8474-0479d7655e86\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hi for all the people who have seen this wonde...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I recently bought the DVD, forgetting just how...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b2f7b98-2210-474c-8474-0479d7655e86')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7b2f7b98-2210-474c-8474-0479d7655e86 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7b2f7b98-2210-474c-8474-0479d7655e86');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e79fc4e4-2ce1-46ba-ae8f-d60af78da406\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e79fc4e4-2ce1-46ba-ae8f-d60af78da406')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e79fc4e4-2ce1-46ba-ae8f-d60af78da406 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49581,\n        \"samples\": [\n          \"It's the old, old story : kids have a party in an old house, demons are unleashed, death and gratuitous nudity ensues. You all know it, it's still a lot of fun.<br /><br />Many people (okay, many horror fans, to be specific) have fond memories of this movie and it's always with slight trepidation that you revisit an old movie to see if it's still as good as you used to think it was. Luckily, this is.<br /><br />It has something for everyone (well, everyone who happens to be male, I suppose). From a fun title sequence to a shoplifting scam involving cunning use of Linnea Quigley's ass to the \\\"mirror\\\" scene to the full on demonic fun, this is a blast from start to finish.<br /><br />The girls are cute, the guys are . . . . male, the death scenes are well done with some good gore effects and, unlike some horrors from the era, this actually keeps a good sense of atmosphere and even tension throughout. Don't get me wrong, it's still fun first and foremost but it offers some nice, freaky moments that should please most genre fans.\",\n          \"It's tempting to view this film as a daring avant-garde experiment. I like to think that the director was trying to see if it was possible to take all the conventions of comedy film and produce something that was completely, utterly, entirely unfunny.<br /><br />The answer, to judge by \\\"From Venus\\\", is a resounding 'Yes'. This may not be the worst film I've ever seen, but my brain seems to have repressed all memory of the others. This horrible flick hovers just on the borderline: bad enough that the thought still causes pain, but not quite so bad that my internal censors have obliterated it from my consciousness.<br /><br />It's difficult for me to imagine what the director and the cast thought they were doing when they made this, or why they went ahead and released it once they'd made it. I doubt anyone involved with it earned very much, but surely between them they could have got together enough money to buy up all the prints and have them burned.<br /><br />This is a movie that has nothing whatsoever to recommend it. It's not even enjoyably bad. It's just a non-movie in which nothing interesting happens. I gave serious thought to taking it back and demanding my money back, which is not something I've ever done before.<br /><br />Don't even think about renting (much less buying!) this horrible non-movie!\",\n          \"Bottom has been my favourite sitcom ever since i saw it on t.v and the movie is even better if your a bottom fan i say this is a must buy!!! the plot is that Eddie and Ritchie run a hotel named guest house Paradise but not all goes right for them as customers leave until a famous actress come to stay they try there best to impress her but not all well go right this is a upbringing to British cinema so buy this and you will wet yourself with laughter. also starring Simon Peggi (shaun of the dead) and also bill neigh (love actually) it might of not done good in the box office or by the looks of it on this website but don't listen to them buy this and i swear you will love it\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vSoYnlADbf3wGATt9Nt7r28tq5ugshktK8a1VxFbNORbh1EJoDptdsluV0f9QSNc8R7TPz26CWDPI5k/pub?output=csv'\n",
        "df_2 = pd.read_csv(url, encoding='utf-8')\n",
        "#df_2.head(5)\n",
        "\n",
        "df = pd.read_csv(url, encoding='utf-8')\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qn3GxmBLQOa"
      },
      "source": [
        "Para entender mejor los datos se debe mencionar que el etiquetado se hace según el siguiente criterio:\n",
        "\n",
        "\n",
        "\n",
        "*   **Positivo (1):** si la película fue calificada con 6 o más estrellas en IMDb\n",
        "*   **Negativo (0):** si la película fue calificada con 5 o menos estrellas en IMDb\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Ya que por comodidad se trabajará con un archivo *CSV* previamente procesado y reindexado aleatoriamente, el autor recomienda como **buena práctica** revisar la dimensión del DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8il7a_WMkAH",
        "outputId": "220b152c-131d-415c-bf0c-fd3ae6bca269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 2)\n",
            "(50000, 2)\n"
          ]
        }
      ],
      "source": [
        "print(df.shape)\n",
        "print(df_2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5-X7D17Mu8i"
      },
      "source": [
        "En ambos casos se cuentan con las 50.000 samples u observaciones y las únicas dos columnas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpJz8fygPLes"
      },
      "source": [
        "# 2. Transformando palabras a **feature vectors**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gILydY-wM6Gf"
      },
      "source": [
        "## 2.1 Modelo **\"Bag-of-words\"** (Bolsa de Palabras)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xM6wVYxNQ6s"
      },
      "source": [
        "Dentro del procesamiento del conjunto de datos se debe convertir la data categórica, como el texto o palabras, en formato numérico antes de aplicar algún algoritmo de ML.\n",
        "\n",
        "En este caso, se usará el modelo \"Bag-of-words\", cuya idea subyacente es la siguiente:\n",
        "\n",
        "\n",
        "1.   Creamos un vocabulario de tokens únicos -por ejemplo, palabras- a partir de todo el conjunto de documentos.\n",
        "2.   Construimos un vector de características (feature vector) a partir de cada documento que contiene los recuentos de la frecuencia con la que cada palabra aparece en el documento en cuestión.\n",
        "\n",
        "---\n",
        "\n",
        "Acá se debe notar que como las palabras en cada documento representan un pequeño subconjunto de toda la **Bolsa de palabras**, la mayoría de entradas del **vector de características** serán nulas, por esto es que podríamos denominarlos vectores **dispersos** (*sparse*).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1YyIQHzPlbp"
      },
      "source": [
        "Para construir la Bolsa de palabras usaremos la clase **CountVectorizer** implementada en scikit-learn.\n",
        "\n",
        "Como se verá, esta clase toma un array de datos de texto, cuyas entradas pueden ser párrafos, textos u oraciones y construye el modelo de Bolsa de palabras respectivo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "c2cVa19DM1nX"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count = CountVectorizer()\n",
        "docs = np.array(['El sol brilla',\n",
        "                 'El clima es agradable',\n",
        "                 'El sol brilla, el clima es agradable',\n",
        "                 'y uno y uno son dos'])\n",
        "\n",
        "bag = count.fit_transform(docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CountVectorizer?"
      ],
      "metadata": {
        "id": "5STGbGGHaZ0Z"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8ypdndhRUmw"
      },
      "source": [
        "Aplicando aquí el método(función) .fit_transform se construye el vocabulario\n",
        "del modelo de bolsa de palabras y transformó esas oraciones en features vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRTIU0reRu1C"
      },
      "source": [
        "Ahora veamos el vocabulario (transcripción) que se realizó:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xHxgcx7RBHt",
        "outputId": "96d4a5e1-72a0-4b92-a184-fe69ee0c9871"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'el': 4,\n",
              " 'sol': 6,\n",
              " 'brilla': 1,\n",
              " 'clima': 2,\n",
              " 'es': 5,\n",
              " 'agradable': 0,\n",
              " 'uno': 8,\n",
              " 'son': 7,\n",
              " 'dos': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "count.vocabulary_ ## Nótese que esto es equivalente a llamar el método .vocabulary  de la clase CountVectorizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujNhoYavSJ9T"
      },
      "source": [
        "Como lo acabamos de ver el vocabulario es almacenado como un diccionario de Python que asigna un entero a cada palabra inédita.\n",
        "\n",
        "Ahora veamos el vector de características creado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxZ-Gf9ZSkaN",
        "outputId": "e5b6fb43-349c-4004-bd1a-15a151a88499"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 0, 1, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 1, 1, 0, 0, 0],\n",
              "       [1, 1, 1, 0, 2, 1, 1, 0, 0],\n",
              "       [0, 0, 0, 1, 0, 0, 0, 1, 2]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "bag.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkPThR73TgT1",
        "outputId": "68a4cbab-244e-4309-eeab-c834179ea2ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9,)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "bag.toarray()[1].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrheL9bvSpco"
      },
      "source": [
        "En estos vectores cada índice mostrado corresponde al entero que se almacenó en el diccionario de palabras (en el CountVectorizer vocabulary).\n",
        "\n",
        "Se pueden hacer las siguientes observaciones:\n",
        "\n",
        "\n",
        "\n",
        "1.   Usualmente los enteros se asignan en orden alfabético según las palabras de la Bolsa.\n",
        "2.   La dimensión de los vectores será $m$x$1$, donde m es el mayor de los enteros asignados en la Bolsa.\n",
        "3. Las entradas de dichos vectores de características se denominan las **\"Frecuencias Brutas de los términos\"**\n",
        "(raw term frequencies) y se representan por $tf(t,d)$.\n",
        "\n",
        "Las frecuencias Brutas de los términos ($tf(t,d)$) miden el número de veces que un término t aparece en un documento(oración) d. Además, no importa el orden en el que aparezcan dichos términos (palabras).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0W9Xy0tVmCG"
      },
      "source": [
        "**Modelos de N-gramas:**\n",
        "\n",
        "La secuencia de elementos del modelo de bolsa de palabras que acabamos de crear también se denomina modelo de 1-grama o unigrama.\n",
        "\n",
        "1-grama o unigrama: cada elemento o token del vocabulario representa una sola palabra.\n",
        "\n",
        "La clase CountVectorizer en scikit-learn nos permite utilizar diferentes modelos de n-gramas a través de su parámetro ngram_range. Aunque por defecto se utiliza una representación de 1 gramo, podríamos cambiar a una representación de 2gramas inicializando una nueva instancia de CountVectorizer con ngram_range=(2,2).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiG7Ei3YVFlr"
      },
      "outputs": [],
      "source": [
        "#CountVectorizer?  ##Se puede llamar la ayuda en cualquier momento para más informacióm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtSLk3TPT6cy"
      },
      "source": [
        "## 2.2 Valoración de la relevancia de las palabras mediante la frecuencia de términos y la frecuencia inversa de documentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SthiMyC5YAn6"
      },
      "source": [
        "Cuando estamos analizando datos de texto, a menudo nos encontramos con palabras que aparecen en varios documentos de ambas clases (reseñas positivas y negativas).\n",
        "\n",
        "Estas palabras que ocurren con frecuencia generalmente no contienen información útil o discriminatoria. En esta subsección, aprenderemos acerca de una técnica útil llamada frecuencia de término-frecuencia inversa de documento (tf-idf) que se puede utilizar para ponderar a la baja esas palabras que ocurren con frecuencia en los vectores de características.\n",
        "\n",
        " El tf-idf se puede definir como el producto de la frecuencia del término y la frecuencia inversa del documento:\n",
        "\n",
        " $$\\text{tf-idf}(t,d)=\\text{tf (t,d)}\\times \\text{idf}(t,d)$$\n",
        "\n",
        " Aquí, tf(t, d) es la frecuencia del término que presentamos en la sección anterior, y la frecuencia inversa del documento idf(t, d) se puede calcular como:\n",
        "\n",
        " $$\\text{idf}(t,d) = \\text{log}\\frac{n_d}{1+\\text{df}(d, t)},$$\n",
        "\n",
        " donde $n_d$ es el número total de documentos y df(d, t) es el número de documentos d que contienen el término t.\n",
        "\n",
        " Nótese que agregar la constante 1 al denominador es opcional y sirve para asignar un valor no nulo a los términos que aparecen en todos los ejemplos de entrenamiento; el logaritmo se utiliza para asegurar que las frecuencias bajas de algunos documentos no se ponderen demasiado.\n",
        "\n",
        "---\n",
        "\n",
        " Scikit-learn implementa otro transformador, el `TfidfTransformer`, que toma las frecuencias de términos crudos de `CountVectorizer` como entrada y las transforma en tf-idfs:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbBfgJyLXfZP",
        "outputId": "a7072957-b68b-4554-808b-8738502738e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.61366674 0.         0.         0.49681612 0.\n",
            "  0.61366674 0.         0.        ]\n",
            " [0.52303503 0.         0.52303503 0.         0.42344193 0.52303503\n",
            "  0.         0.         0.        ]\n",
            " [0.36222092 0.36222092 0.36222092 0.         0.5864981  0.36222092\n",
            "  0.36222092 0.         0.        ]\n",
            " [0.         0.         0.         0.40824829 0.         0.\n",
            "  0.         0.40824829 0.81649658]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf = TfidfTransformer(use_idf=True, ### Activar la reponderación de la frecuencia inversa del documento. Si es False, idf(t) = 1.\n",
        "                         norm='l2', ## Se usa la norma l2 o la euclidiana, entre las opciones está l1(taxista) y l2\n",
        "                         smooth_idf=True) ### Esto incluye sumar un 1 en el numerador y denominador anteriores\n",
        "\n",
        "print(tfidf.fit_transform(count.fit_transform(docs))\n",
        "      .toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHviyJ5iZP5y"
      },
      "outputs": [],
      "source": [
        "#TfidfTransformer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJpmNHtxbEeT"
      },
      "source": [
        "Las ecuaciones para el idf y el tf-idf que se implementaron en scikit-learn son:\n",
        "\n",
        "$$\\text{idf} (t,d) = log\\frac{1 + n_d}{1 + \\text{df}(d, t)}$$\n",
        "\n",
        "$$\\text{tf-idf}(t,d) = \\text{tf}(t,d) \\times (\\text{idf}(t,d)+1)$$\n",
        "\n",
        "Aunque es más común normalizar las frecuencias brutas de los términos antes de calcular los tf-idfs, el `TfidfTransformer` normaliza los tf-idfs directamente.\n",
        "\n",
        "Por defecto (`norm='l2'`), el TfidfTransformer de scikit-learn aplica la normalización L2, que devuelve un vector de longitud 1 dividiendo un vector de características no normalizado *v* por su norma L2:\n",
        "\n",
        "$$v_{\\text{norm}} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v_{1}^{2} + v_{2}^{2} + \\dots + v_{n}^{2}}} = \\frac{v}{\\big (\\sum_{i=1}^{n} v_{i}^{2}\\big)^\\frac{1}{2}}$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E62zTadEcbWo"
      },
      "source": [
        "Veamos un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "VThoQMsOcZuy"
      },
      "outputs": [],
      "source": [
        "count_2 = CountVectorizer()\n",
        "docs_2 = np.array([\n",
        "        'The sun is shining',\n",
        "        'The weather is sweet',\n",
        "        'The sun is shining, the weather is sweet, and one and one is two'])\n",
        "bag_2 = count_2.fit_transform(docs_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9p4EUhzc7LX",
        "outputId": "4fcccf06-915d-48d9-ebd3-8f234b3cf086"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 6,\n",
              " 'sun': 4,\n",
              " 'is': 1,\n",
              " 'shining': 3,\n",
              " 'weather': 8,\n",
              " 'sweet': 5,\n",
              " 'and': 0,\n",
              " 'one': 2,\n",
              " 'two': 7}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "count_2.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIGRPjAGc7V8",
        "outputId": "d0e04721-a860-4003-def5-fc67dc544044"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 1, 1, 0, 1, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 1, 1, 0, 1],\n",
              "       [2, 3, 2, 1, 1, 1, 2, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "bag_2.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4jxK3KIeGcn"
      },
      "source": [
        "La palabra \"is\" tiene una frecuencia de término de 3 (tf = 3) en el documento 3 ($d_3$), y la frecuencia de documento de este término es 3 ya que el término \"is\" aparece en los tres documentos (df = 3). Por lo tanto, podemos calcular el idf de la siguiente manera:\n",
        "\n",
        "$$\\text{idf}(\"is\", d_3) = log \\frac{1+3}{1+3} = 0$$\n",
        "\n",
        "Ahora, para calcular el tf-idf, basta con sumar 1 a la frecuencia inversa del documento y multiplicarlo por la frecuencia del término:\n",
        "$$\\text{tf-idf}(\"is\", d_3)= 3 \\times (0+1) = 3$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkDYjeENdU75",
        "outputId": "c2a07f4b-b22f-471d-8fd9-14c37be79d15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La tf-idf del término (palabra) \"is\" es: 3.00\n"
          ]
        }
      ],
      "source": [
        "tf_is = 3\n",
        "n_docs = 3\n",
        "def tfidf(tf, n_docs):\n",
        "  idf = np.log((n_docs+1) / (3+1))\n",
        "  tfidf = tf_is * (idf + 1)\n",
        "  return tfidf\n",
        "\n",
        "tfidf_is = tfidf(tf_is, n_docs)\n",
        "print(f'La tf-idf del término (palabra) \"is\" es: {tfidf_is:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpbTwW_fhjXB"
      },
      "source": [
        "##2.3 Adecuación datos de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DJX1MCbh6rG"
      },
      "source": [
        "El primer paso importante -antes de construir nuestro modelo de bolsa de palabras- es limpiar los datos de texto\n",
        "eliminando todos los caracteres no deseados. Por ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bdL2roMyhGqO",
        "outputId": "ade998f1-1d86-4911-c349-455151f2b6ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'is seven.<br /><br />Title (Brazil): Not Available'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "df.loc[0, 'review'][-50:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbCuDUMciNbf"
      },
      "source": [
        "Como se pudo ver el texto contiene marcas HTML, así como signos de puntuación y otros caracteres no alfabéticos.\n",
        "\n",
        "Aunque las marcas HTML no contienen mucha semántica útil, los signos de puntuación pueden representar información adicional útil en determinados contextos de PNL.\n",
        "\n",
        "Sin embargo, para simplificar, vamos a\n",
        "eliminar todos los signos de puntuación excepto los emoticones, como :), ya que son útiles para el análisis de sentimientos.\n",
        "\n",
        "---\n",
        "Para realizar esta tarea, utilizaremos la biblioteca de expresiones regulares (regex) de Python, re, como se muestra aquí:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ACKOESeXinXe"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def preprocessor(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
        "                           text)\n",
        "    text = (re.sub('[\\W]+', ' ', text.lower()) + ### El text.lower() convierte en minúscula todo el texto\n",
        "            ' '.join(emoticons).replace('-', ''))\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#re.findall?"
      ],
      "metadata": {
        "id": "MTJf0cVCrPBS"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "PhHaPfqvi_px",
        "outputId": "602c0258-4fd4-4054-f50d-e54515229363"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'is seven title brazil not available'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "preprocessor(df.loc[0, 'review'][-50:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aOLN78b-jEgh",
        "outputId": "7fc94ce6-898b-4c3b-e743-fff98be89b26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this is a test :) :( :)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "preprocessor(\"</a>This :) is :( a test :-)!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q8ZG3LMjRHF"
      },
      "source": [
        "Ahora se aplicará a todo el conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "sjFAB6ZhjT3R"
      },
      "outputs": [],
      "source": [
        "df['review'] = df['review'].apply(preprocessor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLP-TSStj0pW"
      },
      "source": [
        "## 2.4 Procesamiento de documentos en tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y62AI_qOkEkf"
      },
      "source": [
        "Después de preparar con éxito el conjunto de datos de reseñas de películas, ahora debemos pensar en cómo dividir el corpus de texto en elementos individuales. Una forma de tokenizar documentos es dividirlos en palabras individuales al separar los documentos limpios en sus caracteres de espacio en blanco:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMMHig9YkgwQ",
        "outputId": "18edd5a7-e7a6-4852-c61d-871240d2c72b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "def tokenizer(text):\n",
        "    return text.split()\n",
        "\n",
        "tokenizer('runners like running and thus they run')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tapz2XbQkc1D"
      },
      "source": [
        "Otra técnica útil es la derivación de palabras (word stemming), que es el proceso de transformar una palabra en su forma raíz. Esto nos permite asignar palabras relacionadas a la misma raíz. El algoritmo original de derivación de palabras fue desarrollado por Martin F. Porter en 1979 y, por lo tanto, se conoce como el algoritmo de derivación de Porter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcFNl_7hjW9E",
        "outputId": "7deca052-d387-46da-9d3b-f11c57c1fb6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']\n",
            "['en', 'español', 'también', 'funciona', 'felicidad']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "def tokenizer_porter(text):\n",
        "    return [porter.stem(word) for word in text.split()]\n",
        "\n",
        "print(tokenizer_porter('runners like running and thus they run'))\n",
        "print(tokenizer_porter('en españoles también funciona felicidades'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppYDdPi6lkuC"
      },
      "source": [
        "### 2.4.1 Eliminación de palabras vacías (stop words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jq3VWSdlxaq"
      },
      "source": [
        "Las palabras vacías son simplemente aquellas palabras que son extremadamente comunes en todo tipo de textos y probablemente no contienen (o contienen muy poca) información útil que pueda usarse para distinguir entre diferentes clases de documentos. Ejemplos de palabras vacías son \"is\", \"and\", \"has\", y \"like\".\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Para eliminar las palabras vacías de las reseñas de películas, utilizaremos el conjunto de 127 palabras vacías en inglés que está disponible en la biblioteca NLTK, y que se puede obtener llamando a la función `nltk.download`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdo3j6Agl7N0",
        "outputId": "4c7b3f0a-f575-4ba1-b147-6d86ea8ccaef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqCtXSP-lqq_",
        "outputId": "89bf9545-66f1-4d81-d139-4a41e3c1b390"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['runner', 'like', 'run', 'run', 'lot']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "[w for w in tokenizer_porter('a runner likes running and runs a lot')\n",
        " if w not in stop]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYIIio5mmh5i"
      },
      "source": [
        "# 3. Entrenamiento de un modelo de regresión logística para clasificar las reseñas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oojKfjJmu5c"
      },
      "source": [
        "entrenaremos un modelo de regresión logística para clasificar las reseñas de películas en reseñas positivas y negativas basándonos en el modelo de bolsa de palabras. Primero, dividiremos el DataFrame de documentos de texto limpio en 25,000 documentos para entrenamiento (train) y 25,000 documentos para prueba (test):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LCdThVBmuDR"
      },
      "outputs": [],
      "source": [
        "X_train = df.loc[:25000, 'review'].values\n",
        "y_train = df.loc[:25000, 'sentiment'].values\n",
        "X_test = df.loc[25000:, 'review'].values\n",
        "y_test = df.loc[25000:, 'sentiment'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKsq9M2MnQLQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "tfidf = TfidfVectorizer(strip_accents=None,\n",
        "                        lowercase=False,\n",
        "                        preprocessor=None)\n",
        "\n",
        "\"\"\"\n",
        "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
        "               'vect__stop_words': [stop, None],\n",
        "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
        "               'clf__penalty': ['l1', 'l2'],\n",
        "               'clf__C': [1.0, 10.0, 100.0]},\n",
        "              {'vect__ngram_range': [(1, 1)],\n",
        "               'vect__stop_words': [stop, None],\n",
        "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
        "               'vect__use_idf':[False],\n",
        "               'vect__norm':[None],\n",
        "               'clf__penalty': ['l1', 'l2'],\n",
        "               'clf__C': [1.0, 10.0, 100.0]},\n",
        "              ]\n",
        "\"\"\"\n",
        "\n",
        "small_param_grid = [{'vect__ngram_range': [(1, 1)],\n",
        "                     'vect__stop_words': [None],\n",
        "                     'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
        "                     'clf__penalty': ['l2'],\n",
        "                     'clf__C': [1.0, 10.0]},\n",
        "                    {'vect__ngram_range': [(1, 1)],\n",
        "                     'vect__stop_words': [stop, None],\n",
        "                     'vect__tokenizer': [tokenizer],\n",
        "                     'vect__use_idf':[False],\n",
        "                     'vect__norm':[None],\n",
        "                     'clf__penalty': ['l2'],\n",
        "                  'clf__C': [1.0, 10.0]},\n",
        "              ]\n",
        "\n",
        "lr_tfidf = Pipeline([('vect', tfidf),\n",
        "                     ('clf', LogisticRegression(solver='liblinear'))])\n",
        "\n",
        "gs_lr_tfidf = GridSearchCV(lr_tfidf, small_param_grid,\n",
        "                           scoring='accuracy',\n",
        "                           cv=5,\n",
        "                           verbose=1,\n",
        "                           n_jobs=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNxNnxNxnyS6"
      },
      "source": [
        " Para el clasificador de regresión logística, estamos utilizando el solver LIBLINEAR, ya que puede funcionar mejor que la opción predeterminada ('lbfgs') para conjuntos de datos relativamente grandes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mmUUbHroGNC"
      },
      "source": [
        "Nota importante sobre n_jobs\n",
        "\n",
        "Ten en cuenta que se recomienda encarecidamente usar n_jobs=-1 (en lugar de n_jobs=1) en el ejemplo de código anterior para utilizar todos los núcleos disponibles en tu máquina y acelerar la búsqueda en cuadrícula.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tCl6vrZoKhY"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "STPZjgUOnp7a",
        "outputId": "fe6d13da-1208-4478-a9ed-e86be1625c2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n",
              "                                        TfidfVectorizer(lowercase=False)),\n",
              "                                       (&#x27;clf&#x27;,\n",
              "                                        LogisticRegression(solver=&#x27;liblinear&#x27;))]),\n",
              "             n_jobs=-1,\n",
              "             param_grid=[{&#x27;clf__C&#x27;: [1.0, 10.0], &#x27;clf__penalty&#x27;: [&#x27;l2&#x27;],\n",
              "                          &#x27;vect__ngram_range&#x27;: [(1, 1)],\n",
              "                          &#x27;vect__stop_words&#x27;: [None],\n",
              "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x7e57faa54af0&gt;,\n",
              "                                              &lt;function tokenizer_porter at 0x7e57faaa55a0...\n",
              "                          &#x27;vect__stop_words&#x27;: [[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
              "                                                &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;,\n",
              "                                                &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;,\n",
              "                                                &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;,\n",
              "                                                &#x27;yours&#x27;, &#x27;yourself&#x27;,\n",
              "                                                &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;,\n",
              "                                                &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n",
              "                                                &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;,\n",
              "                                                &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
              "                                                &#x27;itself&#x27;, ...],\n",
              "                                               None],\n",
              "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x7e57faa54af0&gt;],\n",
              "                          &#x27;vect__use_idf&#x27;: [False]}],\n",
              "             scoring=&#x27;accuracy&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n",
              "                                        TfidfVectorizer(lowercase=False)),\n",
              "                                       (&#x27;clf&#x27;,\n",
              "                                        LogisticRegression(solver=&#x27;liblinear&#x27;))]),\n",
              "             n_jobs=-1,\n",
              "             param_grid=[{&#x27;clf__C&#x27;: [1.0, 10.0], &#x27;clf__penalty&#x27;: [&#x27;l2&#x27;],\n",
              "                          &#x27;vect__ngram_range&#x27;: [(1, 1)],\n",
              "                          &#x27;vect__stop_words&#x27;: [None],\n",
              "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x7e57faa54af0&gt;,\n",
              "                                              &lt;function tokenizer_porter at 0x7e57faaa55a0...\n",
              "                          &#x27;vect__stop_words&#x27;: [[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
              "                                                &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;,\n",
              "                                                &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;,\n",
              "                                                &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;,\n",
              "                                                &#x27;yours&#x27;, &#x27;yourself&#x27;,\n",
              "                                                &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;,\n",
              "                                                &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n",
              "                                                &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;,\n",
              "                                                &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
              "                                                &#x27;itself&#x27;, ...],\n",
              "                                               None],\n",
              "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x7e57faa54af0&gt;],\n",
              "                          &#x27;vect__use_idf&#x27;: [False]}],\n",
              "             scoring=&#x27;accuracy&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer(lowercase=False)),\n",
              "                (&#x27;clf&#x27;, LogisticRegression(solver=&#x27;liblinear&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(lowercase=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[('vect',\n",
              "                                        TfidfVectorizer(lowercase=False)),\n",
              "                                       ('clf',\n",
              "                                        LogisticRegression(solver='liblinear'))]),\n",
              "             n_jobs=-1,\n",
              "             param_grid=[{'clf__C': [1.0, 10.0], 'clf__penalty': ['l2'],\n",
              "                          'vect__ngram_range': [(1, 1)],\n",
              "                          'vect__stop_words': [None],\n",
              "                          'vect__tokenizer': [<function tokenizer at 0x7e57faa54af0>,\n",
              "                                              <function tokenizer_porter at 0x7e57faaa55a0...\n",
              "                          'vect__stop_words': [['i', 'me', 'my', 'myself', 'we',\n",
              "                                                'our', 'ours', 'ourselves',\n",
              "                                                'you', \"you're\", \"you've\",\n",
              "                                                \"you'll\", \"you'd\", 'your',\n",
              "                                                'yours', 'yourself',\n",
              "                                                'yourselves', 'he', 'him',\n",
              "                                                'his', 'himself', 'she',\n",
              "                                                \"she's\", 'her', 'hers',\n",
              "                                                'herself', 'it', \"it's\", 'its',\n",
              "                                                'itself', ...],\n",
              "                                               None],\n",
              "                          'vect__tokenizer': [<function tokenizer at 0x7e57faa54af0>],\n",
              "                          'vect__use_idf': [False]}],\n",
              "             scoring='accuracy', verbose=1)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gs_lr_tfidf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRHB7dGDo0XD"
      },
      "source": [
        "Cuando inicializamos el objeto GridSearchCV y su cuadrícula de parámetros utilizando el código anterior, nos limitamos a una cantidad limitada de combinaciones de parámetros, ya que la cantidad de vectores de características, así como el gran vocabulario, pueden hacer que la búsqueda en cuadrícula sea computacionalmente costosa. Utilizando una computadora de escritorio estándar, nuestra búsqueda en cuadrícula puede llevar de 5 a 10 minutos para completarse.\n",
        "\n",
        "En el ejemplo de código anterior, reemplazamos CountVectorizer y TfidfTransformer de la subsección anterior con TfidfVectorizer, que combina CountVectorizer con TfidfTransformer. Nuestra param_grid consistió en dos diccionarios de parámetros. En el primer diccionario, usamos TfidfVectorizer con su configuración predeterminada (use_idf=True, smooth_idf=True y norm='l2') para calcular los tf-idfs; en el segundo diccionario, configuramos esos parámetros a use_idf=False, smooth_idf=False y norm=None para entrenar un modelo basado en frecuencias de términos crudas. Además, para el clasificador de regresión logística en sí, entrenamos modelos usando regularización L2 a través del parámetro de penalización y comparamos diferentes fuerzas de regularización definiendo un rango de valores para el parámetro de inversa de regularización C."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NyNk6oJPo2Sf",
        "outputId": "ec4c5990-9c6e-4a54-9192-ff72770799cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x7e57faa54af0>}\n",
            "CV Accuracy: 0.897\n"
          ]
        }
      ],
      "source": [
        "print(f'Best parameter set: {gs_lr_tfidf.best_params_}')\n",
        "print(f'CV Accuracy: {gs_lr_tfidf.best_score_:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zVFjJo1Uo6lu",
        "outputId": "0c47ece8-e7a1-4be0-c0b1-e33b1b908fcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.899\n"
          ]
        }
      ],
      "source": [
        "clf = gs_lr_tfidf.best_estimator_\n",
        "print(f'Test Accuracy: {clf.score(X_test, y_test):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Asignación latente de Dirichlet (LDA)"
      ],
      "metadata": {
        "id": "4mBSvH3_AOqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LDA es un modelo probabilístico generativo que intenta encontrar grupos de palabras que aparecen con frecuencia juntas en diferentes documentos. Estas palabras que aparecen con frecuencia representan nuestros temas, asumiendo que cada documento es una mezcla de diferentes palabras. La entrada a un modelo LDA es el modelo de bolsa de palabras"
      ],
      "metadata": {
        "id": "7DtOMOBlASO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado una matriz de bolsa de palabras como entrada, LDA la descompone en dos nuevas matrices:\n",
        "\n",
        "\n",
        "*   Una matriz de documentos a temas.\n",
        "*   Una matriz de palabras a temas.\n",
        "\n",
        "LDA descompone la matriz de bolsa de palabras de tal manera que si multiplicamos esas dos matrices, podremos reproducir la entrada, la matriz de bolsa de palabras, con el error más bajo posible.\n",
        "\n",
        "En la práctica, estamos interesados en los temas que LDA encontró en la matriz de bolsa de palabras. La única desventaja puede ser que debemos definir el número de temas de antemano; el número de temas es un hiperparámetro de LDA que debe especificarse manualmente."
      ],
      "metadata": {
        "id": "Z7Qzb8zoAh31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA usando scikit-learn"
      ],
      "metadata": {
        "id": "AygzYMQKA5rL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "utilizaremos la clase LatentDirichletAllocation implementada en scikit-learn para descomponer el conjunto de datos de reseñas de películas y categorizarlo en diferentes temas. En el siguiente ejemplo, restringiremos el análisis a 10 temas diferentes"
      ],
      "metadata": {
        "id": "9hQGsRomA6pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count = CountVectorizer(stop_words='english',\n",
        "                        max_df=.1,  #establecimos la frecuencia máxima de documentos de las palabras a considerar en un 10 por ciento\n",
        "                        max_features=5000) #limitamos el número de palabras a considerar a las 5,000 palabras más frecuentes para limitar la dimensionalidad y mejorar las iinferencias\n",
        "X = count.fit_transform(df['review'].values) ## explicar el .values"
      ],
      "metadata": {
        "id": "SisxBpSIARB1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(df['review'].values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx4nh2-0DOjO",
        "outputId": "526a26e3-b8e6-4162-b07b-148b117db610"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9k6yIzT6pTgI"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=10, ### número de temas\n",
        "                                random_state=123,\n",
        "                                learning_method='batch') ## puede ser 'batch' u 'online', donde batch es aprendizaje por lotes, se entrena con todos los datos disponibles y no se actualiza\n",
        "X_topics = lda.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lda.components_.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ol__vAV3Cn2r",
        "outputId": "3eae7b06-ee64-41ac-99e3-68a57280b323"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 5000)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se debe tener en cuenta que los valores de importancia de las palabras están clasificados en orden creciente. Por lo tanto, para imprimir las cinco palabras principales, necesitamos ordenar el array de temas en orden inverso."
      ],
      "metadata": {
        "id": "MBvAsmv6EAUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_top_words = 5\n",
        "feature_names = count.get_feature_names_out()\n",
        "\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    print(f'Topic {(topic_idx + 1)}:')\n",
        "    print(' '.join([feature_names[i]\n",
        "                    for i in topic.argsort()\\\n",
        "                        [:-n_top_words - 1:-1]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20CaitQXECke",
        "outputId": "1b0ec685-da80-4715-a96d-38106915bba2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1:\n",
            "horror effects gore budget killer\n",
            "Topic 2:\n",
            "worst minutes guy money script\n",
            "Topic 3:\n",
            "war american book series history\n",
            "Topic 4:\n",
            "performance beautiful performances excellent feel\n",
            "Topic 5:\n",
            "comedy kids tv family series\n",
            "Topic 6:\n",
            "woman wife father town men\n",
            "Topic 7:\n",
            "space fi sci effects earth\n",
            "Topic 8:\n",
            "music song songs musical dance\n",
            "Topic 9:\n",
            "role version plays original performance\n",
            "Topic 10:\n",
            "action game fight animation series\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "horror = X_topics[:,0 ].argsort()[::-1]\n",
        "\n",
        "for iter_idx, movie_idx in enumerate(horror[:5]):\n",
        "    print(f'\\nPelícula horror #{(iter_idx + 1)}:')\n",
        "    print(df['review'][movie_idx][:300], '...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIac0Pj-EXfC",
        "outputId": "6e6666e3-cf85-449f-8433-46a0ac4217fc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Película horror #1:\n",
            "over christmas break a group of college friends stay behind to help prepare the dorms to be torn down and replaced by apartment buildings to make the work a bit more difficult a murderous chucks wearing psycho is wandering the halls of the dorm preying on the group in various violent ways registered ...\n",
            "\n",
            "Película horror #2:\n",
            "twelve years ago production stopped on the slasher flick hot blooded since almost everyone on the set started dying now a couple of film students have decided to finish the film despite the fact that there s a rumor that the film is cursed well they re about to find out that some curses are real whe ...\n",
            "\n",
            "Película horror #3:\n",
            "i am an avid fan of lucio fulci and yet i must say that zombi 3 aka zombie flesh eaters 2 of 1988 which he made with two other directors bruno mattei and claudi fragasso was quite a disappointment especially compared to its great predecessor fulci s very own gore classic zombi 2 aka zombie felsh eat ...\n",
            "\n",
            "Película horror #4:\n",
            "fulci does this man brings one of the goriest and weirdest movies ever made answer yes cat in the brain also known as nightmare concert is fulci s last masterpiece yes it is no matter what some people will say about it there are few facts why this movie is one of the best fulci s movies fulci make a ...\n",
            "\n",
            "Película horror #5:\n",
            "eight teen convicts are brought to the abandoned blackwell hotel to clean it out as community service they soon discover that it s the residence of a hulking psychopath kane who has a thing for pulling out and collecting eyeballs it doesn t help that the guard watching over them steven vidler has ha ...\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "hBbZQsQKE_73",
        "3V8pUuiVJISc",
        "K9VxtL3_JL2U",
        "ojkwXvc0HPpw",
        "dpJz8fygPLes",
        "QpbTwW_fhjXB",
        "PLP-TSStj0pW",
        "FYIIio5mmh5i",
        "4mBSvH3_AOqi",
        "AygzYMQKA5rL"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}